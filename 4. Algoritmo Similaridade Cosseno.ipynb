{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "affc0ece",
   "metadata": {},
   "source": [
    "## 1. Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1d61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import json\n",
    "import math\n",
    "import nltk\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647ff5de",
   "metadata": {},
   "source": [
    "## 2. Pré-Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b2bb0",
   "metadata": {},
   "source": [
    "### 2.1. Definição da função de Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e40c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(texto):\n",
    "    return \"\".join(c for c in texto if ord(c) < 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b645f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(texto):\n",
    "    return \"\".join(c for c in texto if c.isalnum() or c.isspace())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d9bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(texto):\n",
    "    return \"\".join(c for c in texto if not c.isdigit())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699809c2",
   "metadata": {},
   "source": [
    "A função **preprocessamento** faz a limpeza no texto removendo todos os caracteres que devem ser desconsiderados no processo de geração de tokens de sentenças e palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c84293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de palavras irrelevantes para a etapa de processamento do texto.\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Lista de sinais de pontuação\n",
    "punctuation = string.punctuation\n",
    "\n",
    "def preprocessamento(texto):\n",
    "    texto = texto.lower()\n",
    "    texto = remove_non_ascii(texto)\n",
    "    texto = remove_special_characters(texto)\n",
    "    texto = remove_digits(texto)\n",
    "    \n",
    "    #documento = pln(texto)\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    # Pré-processamento sem a lematização\n",
    "    for token in nltk.word_tokenize(texto):\n",
    "        tokens.append(token)\n",
    "    \n",
    "    # Pré-processamento com a lematização\n",
    "    #for token in documento:\n",
    "        #tokens.append(token.lemma_)\n",
    "\n",
    "    # Filtrar apenas as palavras que não estiverem na lista de stopwords e nem na lista de pontuações\n",
    "    tokens = [palavra for palavra in tokens if palavra not in stopwords and palavra not in punctuation]\n",
    "    \n",
    "    # Converter a lista de palavras para uma String, não adiciona valores numéricos\n",
    "    texto = ' '.join([str(elemento) for elemento in tokens if not elemento.isdigit()])\n",
    "\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af7e0c",
   "metadata": {},
   "source": [
    "### 2.2. Definição das funções de tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3869fe",
   "metadata": {},
   "source": [
    "A função **token_sentences** transforma cada sentença do texto original em um token. Cada frase do texto, é considerado um token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eae0f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma cada frase do texto original em um token.\n",
    "def token_sentences(texto):\n",
    "    sentencas_originais = [sentenca for sentenca in nltk.sent_tokenize(texto)]\n",
    "    \n",
    "    return sentencas_originais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7841bde",
   "metadata": {},
   "source": [
    "A função **preprocessing_sentences** recebe uma lista de sentenças com o texto original e executa a função de preprocessamento, eliminando caracteres especiais, sinais de pontuação, dígitos, stop words, etc. Essa função retorna uma lista de sentenças formatadas (pré-processadas)\n",
    "\n",
    "No algoritmo da similaridade do cosseno, a sentença formatada não pode ser vazia. Nesse caso, acrescenta-se um caractere especial que não influenciará no resultado do algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f14f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processamento das sentenças originais\n",
    "def preprocessing_sentences(sentencas_originais):\n",
    "    sentencas_formatadas = []\n",
    "    for sentenca_original in sentencas_originais:\n",
    "        sentenca_formatada = preprocessamento(sentenca_original)\n",
    "        if sentenca_formatada:\n",
    "            sentencas_formatadas.append(sentenca_formatada)\n",
    "        else:\n",
    "            sentencas_formatadas.append('#')\n",
    "    \n",
    "    return sentencas_formatadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5182f",
   "metadata": {},
   "source": [
    "## 3. Algoritmo de Similaridade do Cosseno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f6eda5",
   "metadata": {},
   "source": [
    "### 3.1. Definição da função sentences_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f360c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_similarity(sentenca1, sentenca2):\n",
    "    \n",
    "    # Transforma cada sentença em uma lista de palavras\n",
    "    palavras1 = [palavra for palavra in nltk.word_tokenize(sentenca1)]\n",
    "    palavras2 = [palavra for palavra in nltk.word_tokenize(sentenca2)]\n",
    "\n",
    "    # Concatena todas as palavras das duas sentenças, desconsiderando as repetições\n",
    "    todas_palavras = list(set(palavras1 + palavras2))\n",
    "\n",
    "    # Criação de vetores com as quantidades de palavras das duas sentenças\n",
    "    # Os vetores são iniciados em todas as posições com o valor zero\n",
    "    vetor1 = [0] * len(todas_palavras)\n",
    "    vetor2 = [0] * len(todas_palavras)\n",
    "\n",
    "    # Quando encontrar uma palavra que esteja na lista, fará um incremento na posição que a palavra ocupa no vetor\n",
    "    # Dessa forma, teremos o número de vezes que cada palavra está presente em cada sentença\n",
    "    for palavra in palavras1:\n",
    "        vetor1[todas_palavras.index(palavra)] += 1\n",
    "    \n",
    "    for palavra in palavras2:\n",
    "        vetor2[todas_palavras.index(palavra)] += 1\n",
    "    \n",
    "    # Cálculo da similaridade entre os dois vetores\n",
    "    return 1 - cosine_distance(vetor1, vetor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102972ae",
   "metadata": {},
   "source": [
    "### 3.2. Definição da função similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4d1ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(sentencas):\n",
    "    \n",
    "    # Inicialização da matriz com todos os valores iguais a zero\n",
    "    matriz_similaridade = np.zeros((len(sentencas), len(sentencas)))\n",
    "    \n",
    "    for i in range(len(sentencas)):\n",
    "        for j in range(len(sentencas)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            matriz_similaridade[i][j] = round(sentences_similarity(sentencas[i], sentencas[j]), 2)\n",
    "\n",
    "    return matriz_similaridade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498cd40f",
   "metadata": {},
   "source": [
    "### 3.3. Definição da função summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c1ca6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(sentencas_originais, sentencas_formatadas, quantidade_sentencas):\n",
    "    \n",
    "    matriz_similaridade = similarity_matrix(sentencas_formatadas)\n",
    "    \n",
    "    # Utilizar a matriz para realizar o cálculo da nota de cada sentença\n",
    "    # Transformar a matriz em um grafo\n",
    "    grafo_similaridade = nx.from_numpy_array(matriz_similaridade)\n",
    "    \n",
    "    # Nós do grafo\n",
    "    #print(grafo_similaridade.nodes)\n",
    "    \n",
    "    # Arestas do grafo\n",
    "    #print(grafo_similaridade.edges)\n",
    "    \n",
    "    # Calcula o ranking dos nós do grafo baseado na estrutura dos links\n",
    "    # A sentença que tiver mais ligações é considerada mais importante\n",
    "    # Pagerank: algoritmo desenvolvido pelo Google para trabalhar com o ranking de páginas web\n",
    "    notas = nx.pagerank(grafo_similaridade)\n",
    "    \n",
    "    # Ordenando as sentenças por notas\n",
    "    notas_ordenadas = sorted(((notas[i], nota) for i, nota in enumerate(sentencas_originais)), reverse=True)\n",
    "    \n",
    "    # Lista com as melhores sentenças\n",
    "    melhores_sentencas = []\n",
    "    for i in range(quantidade_sentencas):\n",
    "        melhores_sentencas.append(notas_ordenadas[i][1])\n",
    "  \n",
    "    return sentencas_originais, melhores_sentencas, notas_ordenadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4739ca17",
   "metadata": {},
   "source": [
    "### 3.4. Definição da função view_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d68db0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_summary(lista_sentencas, melhores_sentencas):\n",
    "    texto = ''\n",
    "    \n",
    "    for sentenca in lista_sentencas:\n",
    "        if sentenca in melhores_sentencas:\n",
    "            texto = texto + ' ' + sentenca\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a774e6",
   "metadata": {},
   "source": [
    "### 3.5. Execução do Algoritmo para todas as notícias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ec1dfb",
   "metadata": {},
   "source": [
    "Esse trecho de código corresponde à execução completa do **Algoritmo de Similaridade do Cosseno** para um lote de notícias. Os seguintes passos são executados:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bc9a8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "      <th>conteudo</th>\n",
       "      <th>sumario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in...</td>\n",
       "      <td>The Daman and Diu administration on Wednesday ...</td>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Malaika slams user who trolled her for 'divorc...</td>\n",
       "      <td>From her special numbers to TVappearances, Bol...</td>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Virgin' now corrected to 'Unmarried' in IGIMS...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaj aapne pakad liya: LeT man Dujana before be...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hotel staff to get training to spot signs of s...</td>\n",
       "      <td>Hotels in Mumbai and other Indian cities are t...</td>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3960</th>\n",
       "      <td>Rasna seeking ?250 cr revenue from snack categ...</td>\n",
       "      <td>Mumbai, Feb 23 (PTI) Fruit juice concentrate m...</td>\n",
       "      <td>Fruit juice concentrate maker Rasna is eyeing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3961</th>\n",
       "      <td>Sachin attends Rajya Sabha after questions on ...</td>\n",
       "      <td>Former cricketer Sachin Tendulkar was spotted ...</td>\n",
       "      <td>Former Indian cricketer Sachin Tendulkar atten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3962</th>\n",
       "      <td>Shouldn't rob their childhood: Aamir on kids r...</td>\n",
       "      <td>Aamir Khan, whose last film Dangal told the st...</td>\n",
       "      <td>Aamir Khan, while talking about reality shows ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963</th>\n",
       "      <td>Asha Bhosle gets ?53,000 power bill for unused...</td>\n",
       "      <td>Maharahstra Power Minister Chandrashekhar Bawa...</td>\n",
       "      <td>The Maharashtra government has initiated an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3964</th>\n",
       "      <td>More than half of India's languages may die in...</td>\n",
       "      <td>More than half of the languages spoken by Indi...</td>\n",
       "      <td>At least 400 languages or more than half langu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3965 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 titulo  \\\n",
       "0     Daman & Diu revokes mandatory Rakshabandhan in...   \n",
       "1     Malaika slams user who trolled her for 'divorc...   \n",
       "2     'Virgin' now corrected to 'Unmarried' in IGIMS...   \n",
       "3     Aaj aapne pakad liya: LeT man Dujana before be...   \n",
       "4     Hotel staff to get training to spot signs of s...   \n",
       "...                                                 ...   \n",
       "3960  Rasna seeking ?250 cr revenue from snack categ...   \n",
       "3961  Sachin attends Rajya Sabha after questions on ...   \n",
       "3962  Shouldn't rob their childhood: Aamir on kids r...   \n",
       "3963  Asha Bhosle gets ?53,000 power bill for unused...   \n",
       "3964  More than half of India's languages may die in...   \n",
       "\n",
       "                                               conteudo  \\\n",
       "0     The Daman and Diu administration on Wednesday ...   \n",
       "1     From her special numbers to TVappearances, Bol...   \n",
       "2     The Indira Gandhi Institute of Medical Science...   \n",
       "3     Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
       "4     Hotels in Mumbai and other Indian cities are t...   \n",
       "...                                                 ...   \n",
       "3960  Mumbai, Feb 23 (PTI) Fruit juice concentrate m...   \n",
       "3961  Former cricketer Sachin Tendulkar was spotted ...   \n",
       "3962  Aamir Khan, whose last film Dangal told the st...   \n",
       "3963  Maharahstra Power Minister Chandrashekhar Bawa...   \n",
       "3964  More than half of the languages spoken by Indi...   \n",
       "\n",
       "                                                sumario  \n",
       "0     The Administration of Union Territory Daman an...  \n",
       "1     Malaika Arora slammed an Instagram user who tr...  \n",
       "2     The Indira Gandhi Institute of Medical Science...  \n",
       "3     Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n",
       "4     Hotels in Maharashtra will train their staff t...  \n",
       "...                                                 ...  \n",
       "3960  Fruit juice concentrate maker Rasna is eyeing ...  \n",
       "3961  Former Indian cricketer Sachin Tendulkar atten...  \n",
       "3962  Aamir Khan, while talking about reality shows ...  \n",
       "3963  The Maharashtra government has initiated an in...  \n",
       "3964  At least 400 languages or more than half langu...  \n",
       "\n",
       "[3965 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Leitura do arquivo JSON com a base de dados de notícias\n",
    "dt = pd.read_json(\"news.json\")\n",
    "display(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40f481c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin: 1635870784.8832512\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "Fim da execução: 1635871784.0270329\n",
      "Duração: 999.1437816619873\n"
     ]
    }
   ],
   "source": [
    "# Lista para armazenar o resultado do processamento do Algoritmo de Similaridade do Cosseno para todas as notícias\n",
    "algoritmo_cosseno = []\n",
    "processamento_cosseno = []\n",
    "\n",
    "inicio = time.time()\n",
    "processamento_cosseno.append({'Begin': inicio})\n",
    "print('Begin:', inicio)\n",
    "\n",
    "for i in range(len(dt)):\n",
    "    titulo = dt.iloc[i, 0]\n",
    "    conteudo = dt.iloc[i, 1]\n",
    "    sumario = dt.iloc[i, 2]\n",
    "    \n",
    "    # Geração de tokens (sentenças) com os textos originais\n",
    "    sentencas_originais = token_sentences(conteudo)\n",
    "    \n",
    "    # Executar o pre-processamento das sentenças para eliminar os caracteres e palavras irrelevantes\n",
    "    sentencas_formatadas = preprocessing_sentences(sentencas_originais)\n",
    "    \n",
    "    # Quantidade de sentenças que irão compor o sumário: 20% do total de sentenças\n",
    "    quantidade_sentencas = math.ceil(len(sentencas_originais)*0.2)\n",
    "    \n",
    "    # Geração das melhores sentenças que irão compor o sumário do texto\n",
    "    sentencas_originais, melhores_sentencas, notas_sentencas = summarize(sentencas_originais, sentencas_formatadas, quantidade_sentencas)\n",
    "    \n",
    "    # Geração do sumário somente com as frases correspondentes às melhores sentenças do texto\n",
    "    sumario_cosine = view_summary(sentencas_originais, melhores_sentencas)\n",
    "    \n",
    "    algoritmo_cosseno.append({'index': i, 'titulo': titulo, 'conteudo': conteudo, 'sumario': sumario, 'sumario_cosine': sumario_cosine})\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "\n",
    "fim = time.time()\n",
    "print('Fim da execução:', fim)\n",
    "processamento_cosseno.append({'End': fim})\n",
    "\n",
    "duracao = fim - inicio\n",
    "print('Duração:', duracao)\n",
    "processamento_cosseno.append({'Duration': duracao})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2addf394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Será criado um arquivo algoritmo_cosseno.json no mesmo diretório de execução do programa\n",
    "arquivo_gravar = os.path.join('algoritmo_cosseno.json')\n",
    "arquivo = open(arquivo_gravar, 'w+')\n",
    "arquivo.write(json.dumps(algoritmo_cosseno, indent=1))\n",
    "arquivo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6feed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Será criado um arquivo processamento_cosseno.json no mesmo diretório de execução do programa\n",
    "arquivo_gravar = os.path.join('processamento_cosseno.json')\n",
    "arquivo = open(arquivo_gravar, 'w+')\n",
    "arquivo.write(json.dumps(processamento_cosseno, indent=1))\n",
    "arquivo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48ef9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
