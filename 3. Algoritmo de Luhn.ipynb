{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "affc0ece",
   "metadata": {},
   "source": [
    "## 1. Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1d61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import json\n",
    "import math\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647ff5de",
   "metadata": {},
   "source": [
    "## 2. Pré-Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b2bb0",
   "metadata": {},
   "source": [
    "### 2.1. Definição da função de Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e40c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(texto):\n",
    "    return \"\".join(c for c in texto if ord(c) < 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b645f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(texto):\n",
    "    return \"\".join(c for c in texto if c.isalnum() or c.isspace())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d9bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(texto):\n",
    "    return \"\".join(c for c in texto if not c.isdigit())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699809c2",
   "metadata": {},
   "source": [
    "A função **preprocessamento** faz a limpeza no texto removendo todos os caracteres que devem ser desconsiderados no processo de geração de tokens de sentenças e palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c84293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessamento(texto):\n",
    "    \n",
    "    # Lista de palavras irrelevantes para a etapa de processamento do texto.\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    # Lista de sinais de pontuação\n",
    "    punctuation = string.punctuation\n",
    "\n",
    "    texto = texto.lower()\n",
    "    texto = remove_non_ascii(texto)\n",
    "    texto = remove_special_characters(texto)\n",
    "    texto = remove_digits(texto)\n",
    "    \n",
    "    #documento = pln(texto)\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    # Pré-processamento sem a lematização\n",
    "    for token in nltk.word_tokenize(texto):\n",
    "        tokens.append(token)\n",
    "    \n",
    "    # Pré-processamento com a lematização\n",
    "    #for token in documento:\n",
    "        #tokens.append(token.lemma_)\n",
    "\n",
    "    # Filtrar apenas as palavras que não estiverem na lista de stopwords e nem na lista de pontuações\n",
    "    tokens = [palavra for palavra in tokens if palavra not in stopwords and palavra not in punctuation]\n",
    "    \n",
    "    # Converter a lista de palavras para uma String, não adiciona valores numéricos\n",
    "    texto = ' '.join([str(elemento) for elemento in tokens if not elemento.isdigit()])\n",
    "\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af7e0c",
   "metadata": {},
   "source": [
    "### 2.2. Definição das funções de tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3869fe",
   "metadata": {},
   "source": [
    "A função **token_sentences** transforma cada sentença do texto original em um token. Cada frase do texto, é considerado um token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eae0f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma cada frase do texto original em um token.\n",
    "def token_sentences(texto):\n",
    "    sentencas_originais = [sentenca for sentenca in nltk.sent_tokenize(texto)]\n",
    "    \n",
    "    return sentencas_originais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7841bde",
   "metadata": {},
   "source": [
    "A função **preprocessing_sentences** recebe uma lista de sentenças com o texto original e executa a função de preprocessamento, eliminando caracteres especiais, sinais de pontuação, dígitos, stop words, etc. Essa função retorna uma lista de sentenças formatadas (pré-processadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f14f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processamento das sentenças originais\n",
    "def preprocessing_sentences(sentencas_originais):\n",
    "    sentencas_formatadas = [preprocessamento(sentenca_original) for sentenca_original in sentencas_originais]\n",
    "    \n",
    "    return sentencas_formatadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5182f",
   "metadata": {},
   "source": [
    "## 3. Algoritmo de Luhn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf0fe96",
   "metadata": {},
   "source": [
    "### 3.1. Definição da função important_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b55749",
   "metadata": {},
   "source": [
    "A função **important_words** recebe uma lista de sentenças pré-processadas e gera uma lista com as palavras mais importantes do texto. É importante observar que não podemos considerar uma quantidade fixa de palavras importantes. O tamanho do texto e q quantidade de palavras distintas que ele possui interferem na quantidade de palavras importantes que devemos selecionar. \n",
    "\n",
    "Este algoritmo considera que número de palavras importantes corresponde a 10% do número total de palavras distintas existentes no texto. \n",
    "\n",
    "Observação: Esse cálculo de 10% é um parâmetro que pode ser ajustado na etapa dos testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc27fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_words(sentencas_formatadas):\n",
    "    \n",
    "    # Transforma cada palavra da sentença formatada em um token\n",
    "    palavras = [palavra for sentenca in sentencas_formatadas for palavra in nltk.word_tokenize(sentenca)]\n",
    "    \n",
    "    # Calcula a frequência de cada palavra\n",
    "    frequencia = nltk.FreqDist(palavras)\n",
    "    \n",
    "    # Considerar 30% das palavras como as mais importantes\n",
    "    total_palavras = len(frequencia)\n",
    "    n_top_palavras = round(0.3 * total_palavras)\n",
    "    \n",
    "    # Lista com as n palavras com maior frequência no texto\n",
    "    top_n_palavras = [palavra[0] for palavra in frequencia.most_common(n_top_palavras)]\n",
    "    \n",
    "    return top_n_palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affe9ed5",
   "metadata": {},
   "source": [
    "### 3.2. Definição da função sentence_grade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df0561",
   "metadata": {},
   "source": [
    "A função **sentence_grade** calcula a nota de cada sentença baseada na frequência de palavras importantes. As sentenças de melhores notas serão selecionadas para fazerem parte do sumário do texto. Essa função retorna uma lista e cada elemento contém o índice da sentença e a nota.\n",
    "\n",
    "A nota da sentença pode ser calculada pela seguinte fórmula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27609f21",
   "metadata": {},
   "source": [
    "$$ \n",
    "Grade = \\frac{IW^2}{GW} \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "IW: Important\\ Words \\\\\n",
    "GW: Total\\ words\\ in\\ group\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6322209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_grade(sentencas, palavras_importantes, distancia):\n",
    "    notas = []\n",
    "    indice_sentenca = 0\n",
    "\n",
    "    # Percorre cada uma das sentenças que passamos como parâmetro e gera uma lista de tokens (palavras)\n",
    "    for sentenca in [nltk.word_tokenize(sentenca.lower()) for sentenca in sentencas]:\n",
    "        indice_palavra = []\n",
    "        \n",
    "        # Percorre a lista de palavras importantes para verificar em qual posição \n",
    "        # de cada sentença essas palavras estão localizadas\n",
    "        for palavra in palavras_importantes:\n",
    "            try:\n",
    "                # Índice onde cada uma das palavras importantes está armazenada na sentença.\n",
    "                indice_palavra.append(sentenca.index(palavra))\n",
    "            # Nem todas as sentenças possuem as palavras importantes. \n",
    "            # A exceção é para quando a palavra não estiver na sentença.\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        # Ordenando a lista de índices de ocorrências de palavras importantes na sentença\n",
    "        indice_palavra.sort()\n",
    "\n",
    "        # Sentenças que não possuem palavras importantes serão ignoradas.\n",
    "        if len(indice_palavra) == 0:\n",
    "            continue\n",
    "\n",
    "        # Quanto mais palavras importantes dentro de um grupo, mais importante é a sentença para o texto\n",
    "        # [0, 1, 3, 5]\n",
    "        lista_grupos = []\n",
    "        grupo = [indice_palavra[0]]\n",
    "        i = 1\n",
    "        while i < len(indice_palavra):\n",
    "            # Verifica a distância entre as palavras importantes\n",
    "            if indice_palavra[i] - indice_palavra[i - 1] < distancia:\n",
    "                grupo.append(indice_palavra[i])\n",
    "            else:\n",
    "                lista_grupos.append(grupo[:])\n",
    "                grupo = [indice_palavra[i]]\n",
    "            i += 1\n",
    "        lista_grupos.append(grupo)\n",
    "\n",
    "        nota_maxima_grupo = 0\n",
    "        for g in lista_grupos:\n",
    "            palavras_importantes_no_grupo = len(g)\n",
    "            total_palavras_no_grupo = g[-1] - g[0] + 1\n",
    "            # Fórmula para calcular a nota da sentença\n",
    "            nota = 1.0 * palavras_importantes_no_grupo**2 / total_palavras_no_grupo\n",
    "\n",
    "            # Cálculo para a maior nota do grupo\n",
    "            if nota > nota_maxima_grupo:\n",
    "                nota_maxima_grupo = nota\n",
    "\n",
    "        # notas: lista com dois elementos: nota_max_grupo e índice da sentença\n",
    "        notas.append((nota_maxima_grupo, indice_sentenca))\n",
    "        indice_sentenca += 1\n",
    "\n",
    "    return notas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef85865",
   "metadata": {},
   "source": [
    "### 3.3. Definição da função summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eefe0d8",
   "metadata": {},
   "source": [
    "A função **summarize** executa os seguintes passos:\n",
    "- Calcula a nota de cada sentença\n",
    "- Gera uma lista com os índices e notas das melhores sentenças.\n",
    "- Gera uma lista com as melhores sentenças descritas no texto original, ou seja, sem as exclusões realizadas na etapa do pré-processamento.\n",
    "\n",
    "Os seguintes parâmetros são passados para a função:\n",
    "- sentencas_originais: lista de tokens (sentenças) com o texto original;\n",
    "- sentencas_formatadas: lista de tokens (sentenças) pré-processadas;\n",
    "- top_n_palavras: lista com as palavras mais importantes do texto\n",
    "- distancia: distância a ser considerada entre as palavras imporantes dentro de um mesmo grupo (valor parametrizável)\n",
    "- quantidade_setencas: Número de sentenças que irão compor o resumo do texto.\n",
    "\n",
    "Observação: Este algoritmo considera que a quantidade de sentenças que irão compor o resumo corresponde a 40% do número total de sentenças do texto. (Esse valor pode ser alterado na etapa dos testes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "049bb882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(sentencas_originais, sentencas_formatadas, top_n_palavras, distancia, quantidade_sentencas):\n",
    "    \n",
    "    # obtem as notas de cada sentença chamando a função calcula_nota_sentenca\n",
    "    notas_sentencas = sentence_grade(sentencas_formatadas, top_n_palavras, distancia)\n",
    "    \n",
    "    # Gera uma lista com os índices e notas das melhores sentenças\n",
    "    lista_indices_melhores_sentencas = []\n",
    "    notas_melhores_sentencas = heapq.nlargest(quantidade_sentencas, notas_sentencas)\n",
    "    for tupla in notas_melhores_sentencas:\n",
    "        lista_indices_melhores_sentencas.append(tupla[1])\n",
    "    lista_indices_melhores_sentencas.sort()\n",
    "    \n",
    "    # Cria a lista de melhores_sentencas com os textos originais\n",
    "    melhores_sentencas = [sentencas_originais[i] for i in lista_indices_melhores_sentencas]\n",
    "    \n",
    "    return sentencas_originais, melhores_sentencas, notas_sentencas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4394212a",
   "metadata": {},
   "source": [
    "### 3.4. Definição da função view_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af0301d",
   "metadata": {},
   "source": [
    "A função **view_summary** exibe o sumário do texto resultante após a aplicação do algoritmo de sumarização. Esta função retorna as melhores sentenças com o texto original e concatenadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d68db0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_summary(lista_sentencas, melhores_sentencas):\n",
    "    texto = ''\n",
    "    \n",
    "    for sentenca in lista_sentencas:\n",
    "        if sentenca in melhores_sentencas:\n",
    "            texto = texto + ' ' + sentenca\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a774e6",
   "metadata": {},
   "source": [
    "### 3.5. Execução do Algoritmo para todas as notícias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ec1dfb",
   "metadata": {},
   "source": [
    "Esse trecho de código corresponde à execução completa do **Algoritmo de Luhn** para um lote de notícias. Os seguintes passos são executados:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bc9a8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "      <th>conteudo</th>\n",
       "      <th>sumario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in...</td>\n",
       "      <td>The Daman and Diu administration on Wednesday ...</td>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Malaika slams user who trolled her for 'divorc...</td>\n",
       "      <td>From her special numbers to TVappearances, Bol...</td>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Virgin' now corrected to 'Unmarried' in IGIMS...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaj aapne pakad liya: LeT man Dujana before be...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hotel staff to get training to spot signs of s...</td>\n",
       "      <td>Hotels in Mumbai and other Indian cities are t...</td>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3960</th>\n",
       "      <td>Rasna seeking ?250 cr revenue from snack categ...</td>\n",
       "      <td>Mumbai, Feb 23 (PTI) Fruit juice concentrate m...</td>\n",
       "      <td>Fruit juice concentrate maker Rasna is eyeing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3961</th>\n",
       "      <td>Sachin attends Rajya Sabha after questions on ...</td>\n",
       "      <td>Former cricketer Sachin Tendulkar was spotted ...</td>\n",
       "      <td>Former Indian cricketer Sachin Tendulkar atten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3962</th>\n",
       "      <td>Shouldn't rob their childhood: Aamir on kids r...</td>\n",
       "      <td>Aamir Khan, whose last film Dangal told the st...</td>\n",
       "      <td>Aamir Khan, while talking about reality shows ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963</th>\n",
       "      <td>Asha Bhosle gets ?53,000 power bill for unused...</td>\n",
       "      <td>Maharahstra Power Minister Chandrashekhar Bawa...</td>\n",
       "      <td>The Maharashtra government has initiated an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3964</th>\n",
       "      <td>More than half of India's languages may die in...</td>\n",
       "      <td>More than half of the languages spoken by Indi...</td>\n",
       "      <td>At least 400 languages or more than half langu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3965 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 titulo  \\\n",
       "0     Daman & Diu revokes mandatory Rakshabandhan in...   \n",
       "1     Malaika slams user who trolled her for 'divorc...   \n",
       "2     'Virgin' now corrected to 'Unmarried' in IGIMS...   \n",
       "3     Aaj aapne pakad liya: LeT man Dujana before be...   \n",
       "4     Hotel staff to get training to spot signs of s...   \n",
       "...                                                 ...   \n",
       "3960  Rasna seeking ?250 cr revenue from snack categ...   \n",
       "3961  Sachin attends Rajya Sabha after questions on ...   \n",
       "3962  Shouldn't rob their childhood: Aamir on kids r...   \n",
       "3963  Asha Bhosle gets ?53,000 power bill for unused...   \n",
       "3964  More than half of India's languages may die in...   \n",
       "\n",
       "                                               conteudo  \\\n",
       "0     The Daman and Diu administration on Wednesday ...   \n",
       "1     From her special numbers to TVappearances, Bol...   \n",
       "2     The Indira Gandhi Institute of Medical Science...   \n",
       "3     Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
       "4     Hotels in Mumbai and other Indian cities are t...   \n",
       "...                                                 ...   \n",
       "3960  Mumbai, Feb 23 (PTI) Fruit juice concentrate m...   \n",
       "3961  Former cricketer Sachin Tendulkar was spotted ...   \n",
       "3962  Aamir Khan, whose last film Dangal told the st...   \n",
       "3963  Maharahstra Power Minister Chandrashekhar Bawa...   \n",
       "3964  More than half of the languages spoken by Indi...   \n",
       "\n",
       "                                                sumario  \n",
       "0     The Administration of Union Territory Daman an...  \n",
       "1     Malaika Arora slammed an Instagram user who tr...  \n",
       "2     The Indira Gandhi Institute of Medical Science...  \n",
       "3     Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n",
       "4     Hotels in Maharashtra will train their staff t...  \n",
       "...                                                 ...  \n",
       "3960  Fruit juice concentrate maker Rasna is eyeing ...  \n",
       "3961  Former Indian cricketer Sachin Tendulkar atten...  \n",
       "3962  Aamir Khan, while talking about reality shows ...  \n",
       "3963  The Maharashtra government has initiated an in...  \n",
       "3964  At least 400 languages or more than half langu...  \n",
       "\n",
       "[3965 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Leitura do arquivo JSON com a base de dados de notícias\n",
    "dt = pd.read_json(\"news.json\")\n",
    "display(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40f481c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin: 1635870314.8275368\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "Fim da execução: 1635870379.4650922\n",
      "Duração: 64.63755536079407\n"
     ]
    }
   ],
   "source": [
    "# Lista para armazenar o resultado do processamento do Algoritmo Luhn para todas as notícias\n",
    "algoritmo_luhn = []\n",
    "processamento_luhn = []\n",
    "\n",
    "inicio = time.time()\n",
    "processamento_luhn.append({'Begin': inicio})\n",
    "print('Begin:', inicio)\n",
    "\n",
    "for i in range(len(dt)):\n",
    "    titulo = dt.iloc[i, 0]\n",
    "    conteudo = dt.iloc[i, 1]\n",
    "    sumario = dt.iloc[i, 2]\n",
    "    \n",
    "    # Geração de tokens (sentenças) com os textos originais\n",
    "    sentencas_originais = token_sentences(conteudo)\n",
    "    \n",
    "    # Executar o pre-processamento das sentenças para eliminar os caracteres e palavras irrelevantes\n",
    "    sentencas_formatadas = preprocessing_sentences(sentencas_originais)\n",
    "    \n",
    "    # Gerar a lista das n palavras mais importantes do texto\n",
    "    top_n_palavras = important_words(sentencas_formatadas)\n",
    "    \n",
    "    # Quantidade de sentenças que irão compor o sumário: 20% do total de sentenças\n",
    "    quantidade_sentencas = math.ceil(len(sentencas_originais)*0.2)\n",
    "    \n",
    "    # Distância entre as palavras importantes no grupo\n",
    "    distancia = 3\n",
    "    \n",
    "    # Geração das melhores sentenças que irão compor o sumário do texto\n",
    "    sentencas_originais, melhores_sentencas, notas_sentencas = summarize(sentencas_originais, sentencas_formatadas, top_n_palavras, distancia, quantidade_sentencas)\n",
    "    \n",
    "    # Geração do sumário somente com as frases correspondentes às melhores sentenças do texto\n",
    "    sumario_luhn = view_summary(sentencas_originais, melhores_sentencas)\n",
    "    \n",
    "    algoritmo_luhn.append({'index': i, 'titulo': titulo, 'conteudo': conteudo, 'sumario': sumario, 'sumario_luhn': sumario_luhn})\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "\n",
    "fim = time.time()\n",
    "print('Fim da execução:', fim)\n",
    "processamento_luhn.append({'End': fim})\n",
    "\n",
    "duracao = fim - inicio\n",
    "print('Duração:', duracao)\n",
    "processamento_luhn.append({'Duration': duracao})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2addf394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Será criado um arquivo algoritmo_luhn.json no mesmo diretório de execução do programa\n",
    "arquivo_gravar = os.path.join('algoritmo_luhn.json')\n",
    "arquivo = open(arquivo_gravar, 'w+')\n",
    "arquivo.write(json.dumps(algoritmo_luhn, indent=1))\n",
    "arquivo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23f8c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Será criado um arquivo processamento_luhn.json no mesmo diretório de execução do programa\n",
    "arquivo_gravar = os.path.join('processamento_luhn.json')\n",
    "arquivo = open(arquivo_gravar, 'w+')\n",
    "arquivo.write(json.dumps(processamento_luhn, indent=1))\n",
    "arquivo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf7af27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
